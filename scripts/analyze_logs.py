"""
This script analyzes the log files generated by the script generate_logs.py or
manually entered.
It extracts relevant information from the log files and generates graphs to
visualize the data.
The extracted data is saved in a CSV file and the graphs are saved as images.
Also a result_text.txt file is created to store the results of the analysis.
"""
import os
import re
import sys
import pandas as pd
import matplotlib.pyplot as plt
from scripts.utils.common import load_config
import seaborn as sns


# LOAD CONFIG
config_path = os.path.join('config', 'config.json')
config = load_config(config_path)

# List of example IP addresses, requests, and HTTP status codes
DEFAULT_LOG_TO_USE = config['LOG_GENERATION_DEFAULT_FILENAME']
RAW_LOGS_PATH = config['RAW_LOGS_PATH']
PROCESSED_LOG_PATH= config['PROCESSED_LOG_PATH']
CSV_PROCESSED_FILE= config['CSV_PROCESSED_FILE']
RESULT_FOLDER = config['RESULT_FOLDER']


def get_datas_from_user_input(log_file_path: str=None) -> pd.DataFrame:
    """
    Get log data from user input and return a DataFrame.

    Args:
        log_file_path (str, optional): The path to the log file to get the data
            from. Defaults to None.

    Returns:
        pandas.DataFrame: DataFrame containing the extracted log data.
    """

    if log_file_path is None:
        # Ask the user for the path to the log file to get the data from
        log_file_path = input("Enter the path to the log file (default: "
                            f"{DEFAULT_LOG_TO_USE}): ")
        if not log_file_path:
            log_file_path = os.path.join(RAW_LOGS_PATH, DEFAULT_LOG_TO_USE)

    # Check if the log file exists
    if not os.path.exists(log_file_path):
        print("The log file does not exist.")
        sys.exit()

    # Read the log file
    with open(log_file_path, 'r', encoding="utf-8") as file:
        data_to_get = file.readlines()

    # Regular expression pattern to extract relevant information
    log_pattern = re.compile(
        r'(?P<ip>\d+\.\d+\.\d+\.\d+) - - \[(?P<date_time>[^\]]+)\] "(?P<request>[^"]+)" (?P<status>\d+) (?P<size>\d+)'
    )

    # Iterate over the log file lines and extract the data
    log_entries = []
    for line in data_to_get:
        match = log_pattern.match(line)
        if match:
            log_entries.append(match.groupdict())

    # Convert datas to a DataFrame obj and return it
    df = pd.DataFrame(log_entries)
    df['status'] = df['status'].astype(int)
    df['size'] = df['size'].astype(int)

    # Clean the date_time column to remove extra spaces
    df['date_time'] = df['date_time'].str.strip()
    df['date_time'] = pd.to_datetime(df['date_time'],
                                     format='%d/%b/%Y:%H:%M:%S')
    return df


def render_result(df: pd.DataFrame) -> None:
    """
    Renders and saves various visualizations and metrics based on the provided
    DataFrame.

    Args:
        df (pandas.DataFrame): The DataFrame containing the log data.

    Returns:
        None
    """
    df.to_csv(os.path.join(PROCESSED_LOG_PATH, CSV_PROCESSED_FILE), index=False)

    # Calculate important metrics to display in result plots and text
    total_requests = len(df)
    status_counts = df['status'].value_counts()
    requests_per_ip = df['ip'].value_counts()
    df['hour'] = df['date_time'].dt.hour  # extract the hour from the date_time
    requests_per_hour = df.groupby(['hour', 'status']).size().unstack()
    avg_size_per_hour = df.groupby('hour')['size'].mean()
    top_ips = requests_per_ip.head(10)


    def save_plot(plot_type: str, title: str, xlabel: str, ylabel: str,
                  filename: str, df: pd.DataFrame = None,
                  top_ips: pd.Series = None,
                  requests_per_hour: pd.DataFrame = None) -> None:
        """
        Closure function to save the plots.

        Args:
            plot_type (str): The type of plot to save.
            title (str): The title of the plot.
            xlabel (str): The label for the x-axis.
            ylabel (str): The label for the y-axis.
            df (pandas.DataFrame, optional): The DataFrame to use for the plot.
            top_ips (pandas.Series, optional): The top IPs to use for the plot.
            requests_per_hour (pandas.DataFrame, optional): The requests per
                hour to use for the plot.

        Returns:
            None

        Raises:
            ValueError: If no data is provided for plotting.
        """

        if df is None and top_ips is None and requests_per_hour is None:
            raise ValueError("No data provided for plotting.")

        if plot_type not in ["countplot", "barplot", "lineplot"]:
            raise ValueError("Invalid plot type.")

        plt.title(title)
        plt.figure(figsize=(10, 6))
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        if plot_type == "countplot":
            sns.countplot(x='status',
                          data=df,
                          palette='viridis',
                          hue="status")
        elif plot_type == "barplot":
            sns.barplot(x=top_ips.values,
                        y=top_ips.index,
                        palette='viridis',
                        hue=requests_per_ip.index)
        elif plot_type == "lineplot":
            for status in status_counts.index:
                sns.lineplot(x=requests_per_hour.index,
                             y=requests_per_hour[status],
                             label=status)

        plt.savefig(filename)
        plt.close()

    # Save needed plots in the results folder
    save_plot("countplot", 'Number of Requests per HTTP Status', 'HTTP Status',
              'Number of Requests', os.path.join(RESULT_FOLDER, 'requests_per_status.png'), df=df)
    save_plot("barplot", 'Top 10 IPs', 'Number of Requests', 'IP',
              os.path.join(RESULT_FOLDER, 'top_ips.png'), top_ips=top_ips)
    save_plot("lineplot", 'Number of Requests per Hour', 'Hour',
              'Number of Requests', os.path.join(RESULT_FOLDER, 'requests_per_hour.png'),
              requests_per_hour=requests_per_hour)

    # Save the important metrics in a text file
    with open(os.path.join(RESULT_FOLDER, 'result_text.txt'), 'w', encoding="utf-8") as file:
        file.write(f"Total requests: {total_requests}\n")
        file.write("Status counts:\n")
        file.write(f"{status_counts}\n")
        file.write(f"Max requests from an IP: {requests_per_ip.max()} from"
                   f" IP {requests_per_ip.idxmax()}\n")
        file.write(f"Min requests from an IP: {requests_per_ip.min()} from IP"
                   f" {requests_per_ip.idxmin()}\n")
        file.write(f"Mean requests from an IP: {requests_per_ip.mean()}\n")
        file.write(f"The hour with the most requests: "
                   f"{requests_per_hour.sum(axis=1).idxmax()} with "
                   f"{requests_per_hour.sum(axis=1).max()} requests\n")
        file.write(f"The hour with the least requests: "
                   f"{requests_per_hour.sum(axis=1).idxmin()} with "
                   f"{requests_per_hour.sum(axis=1).min()} requests\n")
        file.write(f"The hour with the largest average request size: "
                   f"{avg_size_per_hour.idxmax()} with an average size of "
                   f"{avg_size_per_hour.max()} bytes\n")
        file.write(f"The hour with the smallest average request size: "
                   f"{avg_size_per_hour.idxmin()} with an average size of "
                   f"{avg_size_per_hour.min()} bytes\n")


if __name__ == "__main__":

    # extract the data from the log file input by the user
    log_data = get_datas_from_user_input()

    try:
        # Organize the extracted data and render the results
        render_result(log_data)
    except ValueError as e:
        print("\033[91mAn error occurred during the analysis!\033[0m")
        print(e)
    else:
        print("\033[92mAll results have been saved successfully! Check it"
              " out inside the results folder\033[0m")
